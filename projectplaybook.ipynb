{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder, Normalizer, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,ShuffleSplit\n",
    "from joblib import dump, load\n",
    "\n",
    "# import models\n",
    "from sklearn.svm import SVR,SVC,LinearSVC\n",
    "from sklearn.linear_model import LinearRegression,Lasso, Ridge, Perceptron, BayesianRidge, LogisticRegression, LassoCV, SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import StackingRegressor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define python helper Estimators and Transformers to process incoming dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\n",
    "class Back_To_Float(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        This is a Class Used to Preprocess the data, By\n",
    "        encoding N features and filling missing values\n",
    "        too\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, all_features=[\"make_id\", \"model_id\", \"series_id\", \"is_verified_dealer\", \"year_of_manufacture\", \"listingtitle\", \"conditiontitle\", \"sailthru_tag\"],  \n",
    "                        to_encode=[\"make_id\",\"model_id\",\"series_id\",\"is_verified_dealer\",\"year_of_manufacture\",\"listingtitle\", \"conditiontitle\", \"sailthru_tag\"]):\n",
    "    \n",
    "        #Read in data\n",
    "        self.features = all_features\n",
    "        self.to_encode = to_encode\n",
    "\n",
    "    def fit(self,X):\n",
    "        #check if features are present\n",
    "        return self #do nothing\n",
    "\n",
    "    def transform(self,X):\n",
    "        \"\"\"\n",
    "            Work on the dataset\n",
    "        \"\"\"\n",
    "        #check if features are present\n",
    "        try:\n",
    "            X = X[self.features].astype('float')\n",
    "        except Exception as exp:\n",
    "            raise exp\n",
    "        return X\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class Encode_Feature_Label(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        This is a Class Used to Preprocess the data, By\n",
    "        encoding N features and filling missing values\n",
    "        too\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, all_features=[\"make_id\", \"model_id\", \"series_id\", \"is_verified_dealer\", \"year_of_manufacture\", \"listingtitle\", \"conditiontitle\", \"sailthru_tag\"],  \n",
    "                        to_encode=[\"make_id\",\"model_id\",\"series_id\",\"is_verified_dealer\",\"year_of_manufacture\",\"listingtitle\", \"conditiontitle\", \"sailthru_tag\"]):\n",
    "    \n",
    "        #Read in data\n",
    "        self.features = all_features\n",
    "        self.to_encode = to_encode\n",
    "\n",
    "    def fit(self,X):\n",
    "        #check if features are present\n",
    "        try:\n",
    "            X = X[self.features].astype('string')\n",
    "        except Exception as exp:\n",
    "            raise exp\n",
    "\n",
    "        self.all_encode = {each_feature : LabelEncoder().fit(X[each_feature]) for each_feature in self.to_encode}\n",
    "        \n",
    "        #Add 'NaN' to all classes\n",
    "        # for each_feature in self.to_encode:\n",
    "        #     self.all_encode[each_feature].classes_ = list(set(self.all_encode[each_feature].classes_+['NaN']))\n",
    "        return self #do nothing\n",
    "\n",
    "    def transform(self,X):\n",
    "        \"\"\"\n",
    "            Work on the dataset\n",
    "        \"\"\"\n",
    "        #check if features are present\n",
    "        try:\n",
    "            X = X[self.features].astype('string')\n",
    "        except Exception as exp:\n",
    "            raise exp\n",
    "            \n",
    "        #Replace Labels with numerical values\n",
    "        for each_feature in self.to_encode:\n",
    "            classes_ = self.all_encode[each_feature].classes_\n",
    "            \n",
    "            #remove unseen instances\n",
    "            # print(\"class_ \", classes_)\n",
    "            # X[each_feature] = X[each_feature].apply(lambda x: x if x in classes_ else 'NaN')\n",
    "            X[each_feature] = self.all_encode[each_feature].transform(X[each_feature])\n",
    "            \n",
    "            none_index = np.where(classes_ == 'NaN')[0]\n",
    "            if none_index.shape[0] >= 1:\n",
    "                none_index = int(none_index)\n",
    "                X[each_feature].replace(none_index,np.nan,inplace=True)\n",
    "        return X\n",
    "    \n",
    "class Fill_Empty_Spaces_With_Values(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        This is a Class Used to Preprocess the data, By\n",
    "        Filling Missing Values with Standard Values That\n",
    "        Represents Missing Values, e.g numpy.nan.\n",
    "    \"\"\"\n",
    "    def __init__(self, all_features=[\"make_id\", \"model_id\", \"series_id\", \"is_verified_dealer\", \"year_of_manufacture\", \"listingtitle\", \"conditiontitle\", \"sailthru_tag\"],                          \n",
    "                        imputer=None\n",
    "                        ):\n",
    "\n",
    "        #Read in data\n",
    "        self.features = all_features\n",
    "        self.imputer = IterativeImputer(max_iter=20, random_state=0) if not imputer else imputer\n",
    "\n",
    "    def fit(self,X):\n",
    "        try:\n",
    "            X = X[self.features]\n",
    "        except Exception as exp:\n",
    "            raise exp\n",
    "        \n",
    "        self.imputer.fit(X)\n",
    "        return self\n",
    "        \n",
    "    def transform(self,X):\n",
    "        \"\"\"\n",
    "            Work on the dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            X = X[self.features]\n",
    "        except Exception as exp:\n",
    "            raise exp\n",
    "            \n",
    "        #Replace Missing Value With Recognized Missing Value\n",
    "        return pd.DataFrame(self.imputer.transform(X), columns=self.features)\n",
    "        \n",
    "class Fill_Empty_Spaces_With_NaN(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        This is a Class Used to Preprocess the data, By\n",
    "        Filling Missing Values with Standard Values That\n",
    "        Represents Missing Values, e.g numpy.nan.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, all_features=[\"make_id\", \"model_id\", \"series_id\", \"is_verified_dealer\", \"year_of_manufacture\", \"listingtitle\", \"conditiontitle\", \"sailthru_tag\"],\n",
    "                        find_in=[\"listingtitle\", \"conditiontitle\", \"sailthru_tag\"],\n",
    "                                        \n",
    "                        find=None,\n",
    "                        with_=None\n",
    "                        ):\n",
    "    \n",
    "        #Read in data\n",
    "        self.features = all_features\n",
    "        self.find_in = find_in\n",
    "        self.find = ['?','? ',' ?',' ? ','',' ','-',None,'None','none','Null','null',np.nan] if not find else find\n",
    "        self.with_ = np.nan if not with_ else with_\n",
    "\n",
    "    def fit(self,X):\n",
    "        return self #do nothing\n",
    "    def transform(self,X):\n",
    "        \"\"\"\n",
    "            Work on the dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            X = X[self.features]\n",
    "        except Exception as exp:\n",
    "            raise exp\n",
    "            \n",
    "        #Replace Missing Value With Recognized Missing Value\n",
    "        X[self.find_in] = X[self.find_in].replace(self.find,self.with_)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "class Round_Of_Values(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        This is a Class Used to Preprocess the data \n",
    "        by rounding off value to nearest integer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, all_feat=[\"make_id\", \"model_id\", \"series_id\", \"is_verified_dealer\", \"year_of_manufacture\", \"listingtitle\", \"conditiontitle\", \"sailthru_tag\"],           \n",
    "                        feat_to_round=[\"make_id\",\"model_id\",\"series_id\",\"is_verified_dealer\",\"year_of_manufacture\",\"listingtitle\", \"conditiontitle\", \"sailthru_tag\"]):\n",
    "    \n",
    "        #Read in data\n",
    "        self.feat_to_round = feat_to_round\n",
    "        self.all_feat = all_feat\n",
    "\n",
    "    def fit(self,X):\n",
    "        return self #do nothing\n",
    "    \n",
    "    def transform(self,X):\n",
    "        \"\"\"\n",
    "            Round Of Values In Features\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            X = X[self.all_feat]\n",
    "        except Exception as exp:\n",
    "            raise exp\n",
    "            \n",
    "        X[self.feat_to_round] = X[self.feat_to_round].apply(lambda x: round(x)).astype('int')\n",
    "        \n",
    "        return X\n",
    "\n",
    "# class OneHotEncode_Columns(BaseEstimator, TransformerMixin):\n",
    "#     \"\"\"\n",
    "#         This is a Class Used to Preprocess the data by\n",
    "#         one hot encoding of specified features.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, all_feat=[\"make_id\", \"model_id\", \"series_id\", \"is_verified_dealer\", \"year_of_manufacture\", \"listingtitle\", \"conditiontitle\", \"sailthru_tag\"],\n",
    "#                  feat_to_dummy=[\"make_id\",\"model_id\",\"series_id\",\"is_verified_dealer\",\"year_of_manufacture\",\"listingtitle\", \"conditiontitle\", \"sailthru_tag\"]):\n",
    "    \n",
    "#         #Read in data\n",
    "#         self.feat_to_dummy = feat_to_dummy\n",
    "#         self.all_feat = all_feat\n",
    "#     def fit(self,X):\n",
    "#         return self #do nothing\n",
    "    \n",
    "#     def transform(self,X):\n",
    "#         \"\"\"\n",
    "#             One Hot Encode Some Features \n",
    "#         \"\"\"\n",
    "        \n",
    "#         try:\n",
    "#             X = X[self.all_feat]\n",
    "#         except Exception as exp:\n",
    "#             raise exp\n",
    "            \n",
    "#         X = pd.get_dummies(X,columns=self.feat_to_dummy)\n",
    "#         return X\n",
    "\n",
    "class OneHotEncode_Columns(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        This is a Class Used to Preprocess the data by\n",
    "        one hot encoding of specified features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, all_feat=[\"make_id\", \"model_id\", \"series_id\", \"is_verified_dealer\", \"year_of_manufacture\", \"listingtitle\", \"conditiontitle\", \"sailthru_tag\"],\n",
    "                 feat_to_dummy=[\"make_id\",\"model_id\",\"series_id\",\"is_verified_dealer\",\"year_of_manufacture\",\"listingtitle\", \"conditiontitle\", \"sailthru_tag\"]):\n",
    "    \n",
    "        #Read in data\n",
    "        self.feat_to_dummy = feat_to_dummy\n",
    "        self.all_feat = all_feat\n",
    "    def fit(self,X):\n",
    "        try:\n",
    "            X = X[self.all_feat]\n",
    "        except Exception as exp:\n",
    "            raise exp\n",
    "        self.one_hot_encoder = OneHotEncoder().fit(X)\n",
    "        return self #do nothing\n",
    "    \n",
    "    def transform(self,X):\n",
    "        \"\"\"\n",
    "            One Hot Encode Some Features \n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            X = X[self.all_feat]\n",
    "        except Exception as exp:\n",
    "            raise exp\n",
    "        X = self.one_hot_encoder.transform(X)\n",
    "        X = pd.DataFrame(X.toarray(),columns=self.one_hot_encoder.get_feature_names(self.feat_to_dummy))\n",
    "        return X\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define python helper class to process incoming data drom different sources *e.g* *`bodytype`*, *`categories`*, *`condition`*, *`listing`*, *`trueprices`* "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "\n",
    "\n",
    "# Data pipeline to process all incoming dataset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,ShuffleSplit\n",
    "\n",
    "class AutochekDataProcessorPipeline:\n",
    "    def __init__(\n",
    "                    self, path_to_body_type=None, path_to_categories=None, \n",
    "                    path_to_condition=None, path_to_listing=None, path_to_trueprices=None, \n",
    "                    features_to_extract=[\"make_id\", \"model_id\", \"series_id\", \"is_verified_dealer\", \"year_of_manufacture\", \"listingtitle\", \"conditiontitle\", \"sailthru_tag\"],\n",
    "                    find_missing_int=[\"make_id\", \"model_id\", \"series_id\", \"is_verified_dealer\",\"year_of_manufacture\"],\n",
    "                    find_missing_str=[\"listingtitle\", \"conditiontitle\", \"sailthru_tag\"],\n",
    "                    feature_to_dummy=[\"make_id\",\"model_id\",\"series_id\",\"is_verified_dealer\",\"year_of_manufacture\",\"listingtitle\", \"conditiontitle\", \"sailthru_tag\"],\n",
    "                    target_column=[\"price\"]\n",
    "                ):\n",
    "        \"\"\"Init all dataset\"\"\"\n",
    "        \n",
    "        self.set_paths_nd_extras(path_to_body_type, path_to_categories, path_to_condition, path_to_listing, path_to_trueprices, features_to_extract, find_missing_int, find_missing_str, feature_to_dummy, target_column)\n",
    "        self.reset_attributes()\n",
    "        self.load_dataset_from_diff_loc()\n",
    "\n",
    "    def set_paths_nd_extras(self, path_to_body_type, path_to_categories, path_to_condition, path_to_listing, path_to_trueprices, features_to_extract, find_missing_int, find_missing_str, feature_to_dummy, target_column):\n",
    "        \"\"\"This method sets the path to all the link to get the data from\"\"\"\n",
    "        self.path_to_body_type = path_to_body_type\n",
    "        self.path_to_categories = path_to_categories\n",
    "        self.path_to_condition = path_to_condition\n",
    "        self.path_to_listing = path_to_listing\n",
    "        self.path_to_trueprices = path_to_trueprices\n",
    "        self.features_to_extract = features_to_extract\n",
    "        self.find_missing_int = find_missing_int \n",
    "        self.find_missing_str = find_missing_str\n",
    "        self.feature_to_dummy = feature_to_dummy\n",
    "        self.target_column = target_column\n",
    "        \n",
    "        \n",
    "    def get_paths_nd_extras(self):\n",
    "        \"\"\"This method gets the set paths from the object\"\"\"\n",
    "        return (\n",
    "                        self.path_to_body_type, self.path_to_categories, self.path_to_condition, \n",
    "                        self.path_to_listing, self.path_to_trueprices, self.features_to_extract, \n",
    "                        self.find_missing_int, self.find_missing_str, \n",
    "                        self.feature_to_dummy,self.target_column\n",
    "                )\n",
    "    \n",
    "    def get_features(self):\n",
    "        return self.features_to_extract\n",
    "    \n",
    "    def get_target(self):\n",
    "        return self.target_column\n",
    "    \n",
    "    def get_missing_int_features(self):\n",
    "        return self.find_missing_int\n",
    "    \n",
    "    def reset_attributes(self):\n",
    "        \"\"\"This method resets all the data attributes in the created object\"\"\"\n",
    "        self.body_type_df = None\n",
    "        self.categories_df = None\n",
    "        self.condition_df = None\n",
    "        self.listing_df = None\n",
    "        self.trueprices_df = None\n",
    "        self.dataset_output = None\n",
    "        self.output_columns = None\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.val_df = None\n",
    "        self.currentX = None\n",
    "        self.currentY = None\n",
    "        \n",
    "        # init pipeline flags\n",
    "        self.normalizer_is_fitted = False\n",
    "        self.pipeline_feature1_is_fitted = False\n",
    "        self.pipeline_feature2_is_fitted = False\n",
    "        self.pipeline_target_is_fitted = False\n",
    "        \n",
    "        #Initialize Pipeline for features\n",
    "        self.process_pipeline_feature1 = Pipeline([\n",
    "                        ('fill_missing_with_NaN', Fill_Empty_Spaces_With_NaN(all_features=self.get_features(),find_in=self.get_missing_int_features()+self.find_missing_str,with_='NaN')),\n",
    "                        ('encode_cat_fea', Encode_Feature_Label(all_features=self.get_features(), to_encode=self.feature_to_dummy)),\n",
    "                        ('int_column_to_float', Back_To_Float(all_features=self.get_features(), to_encode=self.get_missing_int_features())),\n",
    "                        ('fill_missing_for_nan', Fill_Empty_Spaces_With_NaN(all_features=self.get_features(),find_in=self.get_missing_int_features()+self.find_missing_str,with_=np.nan)),\n",
    "                        ('Mice_Imputer', Fill_Empty_Spaces_With_Values(all_features=self.get_features())),\n",
    "                        ('Round_of_Values', Round_Of_Values(all_feat=self.get_features(),feat_to_round=self.feature_to_dummy))\n",
    "                        ]) \n",
    "        \n",
    "        #Initialize Pipeline for features\n",
    "        self.process_pipeline_feature2 = Pipeline([\n",
    "                        ('one_hot_encode', OneHotEncode_Columns(all_feat=self.get_features(), feat_to_dummy=self.feature_to_dummy))\n",
    "                        ]) \n",
    "        \n",
    "        #Initialize Pipeline fore target\n",
    "        self.process_pipeline_target = Pipeline([\n",
    "                        ('fill_missing_target', Fill_Empty_Spaces_With_NaN(all_features=self.get_target(),find_in=self.get_target(),with_=np.nan)),\n",
    "                        ('Mice_Imputer_target', Fill_Empty_Spaces_With_Values(all_features=self.get_target())),\n",
    "                        ]) \n",
    "        \n",
    "        #Initialize Normalizer\n",
    "        self.normalizer = Normalizer()\n",
    "             \n",
    "    def load_dataset_from_diff_loc(self):\n",
    "        \"\"\"This method loads all the dataset from their different path into a pandas dataframe\"\"\"\n",
    "        self.body_type_df = pd.read_csv(self.path_to_body_type,sep=\";\") if self.path_to_body_type else None\n",
    "        self.categories_df = pd.read_csv(self.path_to_categories,sep=\";\") if self.path_to_categories else None\n",
    "        self.condition_df = pd.read_csv(self.path_to_condition,sep=\";\") if self.path_to_condition else None\n",
    "        self.listing_df = pd.read_csv(self.path_to_listing,sep=\";\") if self.path_to_listing else None\n",
    "        self.trueprices_df = pd.read_csv(self.path_to_trueprices,sep=\";\") if self.path_to_trueprices else None\n",
    "        \n",
    "    def _transformer(self, data, steps, normalize):\n",
    "        assert self.pipeline_target_is_fitted,\"Pipeline Feature1 not fitted please fit\"\n",
    "        target = self.process_pipeline_target.transform(data[self.get_target()])\n",
    "        \n",
    "        assert self.pipeline_feature1_is_fitted, \"Pipeline Feature1 not fitted please fit\"\n",
    "        data = self.process_pipeline_feature1.transform(data[self.get_features()])\n",
    "        \n",
    "        if steps==\"all\":\n",
    "            assert self.pipeline_feature2_is_fitted, \"Pipeline Feature2 not fitted please fit\"\n",
    "            data = self.process_pipeline_feature2.transform(data[self.get_features()])\n",
    "            \n",
    "            if normalize:\n",
    "                assert self.normalizer_is_fitted, \"Normalizer not fitted please fit\"\n",
    "                data = self.normalizer.transform(csr_matrix(data))\n",
    "                target = target.values\n",
    "                \n",
    "        return data, target\n",
    "        \n",
    "    def pipeline_transform(self, data=\"original\",mode=None, steps=\"all\", return_result=True, normalize=True):\n",
    "        \"\"\"To trsnsform data\n",
    "        data: One of 4 options ('original', 'trainset', 'testset', data) \n",
    "        mode: 'train' for train mode, 'test' for test mode\n",
    "        step: 'all' to follow all transform steps, 'skip2' to skip second step(one-hot encoding step)\n",
    "        return_result(bool): to return processed data\n",
    "        \"\"\"\n",
    "        if mode:\n",
    "            self.set_mode(mode=mode)\n",
    "        if type(data) == type(\"train\") and data in [\"original\",\"trainset\", \"testset\", \"valset\"]:\n",
    "            if data==\"original\":\n",
    "                assert type(self.dataset_output) != type(None), \"original dataset is none, please run the .process_dataset method\"\n",
    "                self.currentX, self.currentY = self._transformer(data=self.dataset_output, steps=steps, normalize=normalize)\n",
    "                if return_result:\n",
    "                    return self.currentX, self.currentY\n",
    "            \n",
    "            elif data==\"trainset\":\n",
    "                assert type(self.train_df) != type(None), \"trainset is none, please run the .split_data method\"\n",
    "                self.currentX, self.currentY = self._transformer(data=self.train_df, steps=steps, normalize=normalize)\n",
    "                if return_result:\n",
    "                    return self.currentX, self.currentY\n",
    "                            \n",
    "            elif data==\"testset\":\n",
    "                assert type(self.test_df) != type(None), \"testset is none, please run the .split_data method\"\n",
    "                self.currentX, self.currentY = self._transformer(data=self.test_df, steps=steps, normalize=normalize)\n",
    "                if return_result:\n",
    "                    return self.currentX, self.currentY\n",
    "                            \n",
    "            elif data==\"valset\":\n",
    "                assert type(self.val_df) != type(None), \"valset is none, please run the .split_data method\"\n",
    "                self.currentX, self.currentY = self._transformer(data=self.val_df, steps=steps, normalize=normalize)\n",
    "                if return_result:\n",
    "                    return self.currentX, self.currentY\n",
    "        else:\n",
    "            assert type(data)==type(pd.DataFrame()), \"Data has to be a type dataframe or one of the following strings 'trainset', 'testset', 'valset'.\"\n",
    "            self.currentX, self.currentY = self._transformer(data=data, steps=steps, normalize=normalize)\n",
    "            if return_result:\n",
    "                return self.currentX, self.currentY\n",
    "\n",
    "    def _fitter(self, data, steps, only_normalize):\n",
    "        if only_normalize:\n",
    "            # target = data[self.get_target()].values\n",
    "            data, target = self._transformer(data=data, steps=steps, normalize=False)\n",
    "            target = target.values\n",
    "            data = self.normalizer.fit_transform(csr_matrix(data))\n",
    "            self.normalizer_is_fitted = True\n",
    "        else:\n",
    "            target = self.process_pipeline_target.fit_transform(data[self.get_target()])\n",
    "            data = self.process_pipeline_feature1.fit_transform(data[self.get_features()])\n",
    "            self.pipeline_feature1_is_fitted = True\n",
    "            self.pipeline_target_is_fitted = True\n",
    "            \n",
    "            if steps==\"all\":\n",
    "                data = self.process_pipeline_feature2.fit_transform(data[self.get_features()])\n",
    "                self.pipeline_feature2_is_fitted = True\n",
    "        return data, target\n",
    "    \n",
    "\n",
    "    def pipeline_fit(self, data=\"original\", steps=\"all\", return_result=True, only_normalize=True):\n",
    "        \"\"\"To trsnsform data\n",
    "        data: One of 4 options ('original', 'trainset', 'testset', data) \n",
    "        step: 'all' to follow all transform steps, 'skip2' to skip second step(one-hot encoding step)\n",
    "        return_result(bool): to return processed data\n",
    "        only_normalize(True|False) \n",
    "        \"\"\"\n",
    "        if type(data) == type(\"train\") and data in [\"original\",\"trainset\", \"testset\", \"valset\"]:\n",
    "            if data==\"original\":\n",
    "                assert type(self.dataset_output) != type(None), \"original dataset is none, please run the .process_dataset method\"\n",
    "                self.currentX, self.currentY = self._fitter(data=self.dataset_output, steps=steps, only_normalize=only_normalize)\n",
    "                if return_result:\n",
    "                    return self.currentX, self.currentY\n",
    "            \n",
    "            elif data==\"trainset\":\n",
    "                assert type(self.train_df) != type(None), \"trainset is none, please run the .split_data method\"\n",
    "                self.currentX, self.currentY = self._fitter(data=self.train_df, steps=steps, only_normalize=only_normalize)\n",
    "                if return_result:\n",
    "                    return self.currentX, self.currentY\n",
    "                            \n",
    "            elif data==\"testset\":\n",
    "                assert type(self.test_df) != type(None), \"testset is none, please run the .split_data method\"\n",
    "                self.currentX, self.currentY = self._fitter(data=self.test_df, steps=steps, only_normalize=only_normalize)\n",
    "                if return_result:\n",
    "                    return self.currentX, self.currentY\n",
    "                            \n",
    "            elif data==\"valset\":\n",
    "                assert type(self.val_df) != type(None), \"valset is none, please run the .split_data method\"\n",
    "                self.currentX, self.currentY = self._fitter(data=self.val_df, steps=steps, only_normalize=only_normalize)\n",
    "                if return_result:\n",
    "                    return self.currentX, self.currentY\n",
    "        else:\n",
    "            assert type(data)==type(pd.DataFrame()), \"Data has to be a type dataframe or one of the following strings 'trainset', 'testset', 'valset'.\"\n",
    "            self.currentX, self.currentY = self._fitter(data=data, steps=steps, only_normalize=only_normalize)\n",
    "            if return_result:\n",
    "                return self.currentX, self.currentY\n",
    "\n",
    "    def process_dataset(self):\n",
    "        \"\"\"This method joins the datasets from the different data sources into one data for training purpose\"\"\"\n",
    "        # Process dataset by joining table exactly the way it was done using sql\n",
    "        if type(self.output_columns) != type(None):\n",
    "            self.reset_attributes()\n",
    "            self.set_paths_nd_extras(*self.get_paths_nd_extras())\n",
    "            self.load_dataset_from_diff_loc()\n",
    "            \n",
    "        assert type(self.trueprices_df) != type(None), \"trueprices_df can not be None, please specify a location to get this data from.\"\n",
    "        assert type(self.listing_df) != type(None), \"listing_df can not be None, please specify a location to get this data from.\"\n",
    "        assert type(self.condition_df) != type(None), \"condition_df can not be None, please specify a location to get this data from.\"\n",
    "        assert type(self.body_type_df) != type(None), \"body_type_df can not be None, please specify a location to get this data from.\"\n",
    "        assert type(self.categories_df) != type(None), \"categories_df can not be None, please specify a location to get this data from.\"\n",
    "        \n",
    "        #left join on listing_df\n",
    "        self.trueprices_df[[\"listingtitle\"]] = self.trueprices_df.merge(self.listing_df, left_on=\"listing_id\", right_on=\"id\", how=\"left\")[[\"title\"]].copy()\n",
    "        \n",
    "        #left join on condition_df\n",
    "        self.trueprices_df[[\"conditiontitle\"]] = self.trueprices_df.merge(self.condition_df, left_on=\"condition_type_id\", right_on=\"id\", how=\"left\")[[\"title\"]].copy()\n",
    "        \n",
    "        #left join on body_type_id\n",
    "        self.trueprices_df[[\"sailthru_tag\"]] = self.trueprices_df.merge(self.body_type_df, left_on=\"body_type_id\", right_on=\"id\", how=\"left\")[[\"sailthru_tag\"]].copy()\n",
    "        \n",
    "        # extract columns we are interested in\n",
    "        self.dataset_output = self.trueprices_df[[\"id\", \"make_id\", \"model_id\", \"series_id\", \"is_verified_dealer\", \"price\", \"year_of_manufacture\", \"listingtitle\", \"conditiontitle\", \"sailthru_tag\"]].copy()\n",
    "        self.output_columns = self.dataset_output.columns\n",
    "        self.dataset_output = Fill_Empty_Spaces_With_NaN(all_features=self.output_columns,find_in=self.output_columns,with_=\"NaN\").fit_transform(X=self.dataset_output)\n",
    "    \n",
    "    def split_data(self, split_data_train_test = True, split_data_test_val = True, train_test_ratio=0.8, test_val_ratio=0.5, return_split=False, random_seed=42):\n",
    "        \"\"\"This method splits dataset into train-test or train-test-val\"\"\"\n",
    "        if split_data_train_test:\n",
    "            (X_train,y_train), (X_test, y_test) = self.Split_Datato_Half(X=self.dataset_output, y=self.dataset_output[[\"price\"]], train_ratio=train_test_ratio, Stratified=False, random_seed=random_seed)\n",
    "            if split_data_test_val:\n",
    "                (X_val,y_val), (X_test, y_test) = self.Split_Datato_Half(X=X_test, y=y_test, train_ratio=test_val_ratio, Stratified=False, random_seed=random_seed)\n",
    "                self.train_df, self.test_df, self.val_df = pd.DataFrame(X_train, columns=self.output_columns), pd.DataFrame(X_test, columns=self.output_columns), pd.DataFrame(X_val, columns=self.output_columns)\n",
    "                if return_split:\n",
    "                    return self.train_df, self.test_df, self.val_df\n",
    "            else:\n",
    "                self.train_df, self.test_df = pd.DataFrame(X_train, columns=self.output_columns), pd.DataFrame(X_test, columns=self.output_columns)\n",
    "                if return_split:\n",
    "                    return self.train_df, self.test_df\n",
    "        else:\n",
    "            return None\n",
    "    # def save_original_processed_data(self,original_processed_data=\"original_processed_data.csv\"):\n",
    "    #     \"\"\"This method saves the result of the merge from all data sources\"\"\"\n",
    "    #     assert type(self.dataset_output) != type(None), \"dataset output can not be none, please run the `.process_dataset` method\"\n",
    "    #     self.dataset_output.to_csv(original_processed_data or \"original_processed_data.csv\")\n",
    "        \n",
    "    def save_splits(\n",
    "                            self, \n",
    "                            save_ordinary_processed_data = False,\n",
    "                            save_train_data = False, save_test_data = False, \n",
    "                            save_val_data = False, train_data_filename =\"./data/processed/train.csv\", \n",
    "                            ordinary_processed_data_filename =\"./data/processed/ordinary_processed_data.csv\", \n",
    "                            test_data_filename = \"./data/processed/test.csv\", val_data_filename = \"./data/processed/val.csv\",\n",
    "                    ):\n",
    "        \"\"\"This method saves the train, test or val split\"\"\"\n",
    "        if save_ordinary_processed_data:\n",
    "            assert type(self.dataset_output) != type(None), \"dataset output can not be none, please run the `.process_dataset` method.\"\n",
    "            self.dataset_output.to_csv(ordinary_processed_data_filename or \"./data/processed/ordinary_processed_data.csv\",index=False)\n",
    "            \n",
    "        if save_train_data:\n",
    "            assert type(self.train_df) != type(None), \"train_df can not be none, please run the `.split_data` method.\"\n",
    "            self.train_df.to_csv(train_data_filename or \"./data/processed/train.csv\",index=False)\n",
    "            \n",
    "        if save_test_data:\n",
    "            assert type(self.test_df) != type(None), \"test_df can not be none, please run the `.split_data` method.\"\n",
    "            self.test_df.to_csv(test_data_filename or \"./data/processed/test.csv\",index=False)\n",
    "            \n",
    "        if save_val_data:\n",
    "            assert type(self.val_df) != type(None), \"val_df can not be none, please run the `.split_data` method, setting split_data_test_val=True.\"\n",
    "            self.val_df.to_csv(val_data_filename or \"./data/processed/val.csv\",index=False)\n",
    "            \n",
    "        \n",
    "    def load_splits(\n",
    "                            self, \n",
    "                            load_ordinary_processed_data = False,\n",
    "                            load_train_data = False, load_test_data = False, \n",
    "                            load_val_data = False, train_data_filename =\"./data/processed/train.csv\", \n",
    "                            ordinary_processed_data_filename =\"./data/processed/ordinary_processed_data.csv\", \n",
    "                            test_data_filename = \"./data/processed/test.csv\", val_data_filename = \"./data/processed/val.csv\",\n",
    "                    ):\n",
    "        \"\"\"This method saves the train, test or val split\"\"\"\n",
    "        if load_ordinary_processed_data:\n",
    "            self.dataset_output = pd.read_csv(ordinary_processed_data_filename or \"./data/processed/ordinary_processed_data.csv\")\n",
    "            \n",
    "        if load_train_data:\n",
    "            self.train_df = pd.read_csv(train_data_filename or \"./data/processed/train.csv\")\n",
    "            \n",
    "        if load_test_data:\n",
    "            self.test_df = pd.read_csv(test_data_filename or \"./data/processed/test.csv\")\n",
    "            \n",
    "        if load_val_data:\n",
    "            self.val_df = pd.read_csv(val_data_filename or \"./data/processed/val.csv\")\n",
    "            \n",
    "            \n",
    "                        \n",
    "    def save_pipelines_nd_normalizer(self, filename=\"./pipeline_nd_normalizer/pipelines_nd_normalizer\"):\n",
    "        dump({\n",
    "                \"process_pipeline_feature1\":self.process_pipeline_feature1,\n",
    "                \"process_pipeline_feature2\":self.process_pipeline_feature2,\n",
    "                \"process_pipeline_target\": self.process_pipeline_target,\n",
    "                \"normalizer\": self.normalizer,\n",
    "                \"normalizer_is_fitted\":self.normalizer_is_fitted,\n",
    "                \"pipeline_feature1_is_fitted\":self.pipeline_feature1_is_fitted,\n",
    "                \"pipeline_feature2_is_fitted\":self.pipeline_feature2_is_fitted,\n",
    "                \"pipeline_target_is_fitted\":self.pipeline_target_is_fitted,\n",
    "        }, filename or \"./pipeline_nd_normalizer/pipelines_nd_normalizer\")\n",
    "    \n",
    "    def load_pipeline_nd_normalizer(self, filename=\"./pipeline_nd_normalizer/pipelines_nd_normalizer\"):\n",
    "        pipelines_nd_normalizer = load(filename or \"./pipeline_nd_normalizer/pipelines_nd_normalizer\")\n",
    "        self.process_pipeline_feature1 = pipelines_nd_normalizer[\"process_pipeline_feature1\"]\n",
    "        self.process_pipeline_feature2 = pipelines_nd_normalizer[\"process_pipeline_feature2\"]\n",
    "        self.process_pipeline_target = pipelines_nd_normalizer[\"process_pipeline_target\"]\n",
    "        self.normalizer = pipelines_nd_normalizer[\"normalizer\"]\n",
    "        self.pipeline_feature1_is_fitted = pipelines_nd_normalizer[\"pipeline_feature1_is_fitted\"]\n",
    "        self.pipeline_feature2_is_fitted = pipelines_nd_normalizer[\"pipeline_feature2_is_fitted\"]\n",
    "        self.pipeline_target_is_fitted = pipelines_nd_normalizer[\"pipeline_target_is_fitted\"]\n",
    "        self.normalizer_is_fitted = pipelines_nd_normalizer[\"normalizer_is_fitted\"]\n",
    "                \n",
    "    @classmethod\n",
    "    def Split_Datato_Half(cls,X,y,train_ratio=0.8,Stratified=False, random_seed=42):\n",
    "        \"\"\"\n",
    "            This Function Utilizes the Split Functions in Sklearn \n",
    "            to Split that into Two halves.\n",
    "        \"\"\"\n",
    "        supported = [np.ndarray, pd.core.frame.DataFrame]\n",
    "        if type(X) not in supported or type(y) not in supported: \n",
    "            raise ValueError(f'X is {type(X)} and y is {type(y)}, both values are expected to be either numpy array or a pandas dataframe')\n",
    "\n",
    "        split_data = StratifiedShuffleSplit(n_splits=1, train_size=train_ratio, random_state=random_seed) if Stratified else ShuffleSplit(n_splits=1, train_size=train_ratio ,random_state=random_seed)\n",
    "        \n",
    "        #split the data into two halves\n",
    "        try:\n",
    "            X,y = X.values, y.values\n",
    "        except:\n",
    "            X,y = X,y\n",
    "\n",
    "        for train_index, test_index in split_data.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "        return (X_train,y_train), (X_test, y_test)\n",
    "        \n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define model class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "from joblib import dump, load\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression,Lasso, Ridge, Perceptron, BayesianRidge, LogisticRegression, SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "\n",
    "class AutochekModel:\n",
    "    _supported_backbones = {\n",
    "            'svr':SVR(),\n",
    "            'lr':LinearRegression(),\n",
    "            'la':Lasso(),\n",
    "            'rg':Ridge(),\n",
    "            'sgd':SGDRegressor(),\n",
    "            'kn':KNeighborsRegressor()\n",
    "            }\n",
    "    \n",
    "    def __init__(self, model_back_bone=None) -> None:\n",
    "        # init model back bone\n",
    "        self.model_back_bone = model_back_bone\n",
    "        self.current_rmse = None\n",
    "        \n",
    "    def compute_rmse(self,X,y, return_rmse=True):\n",
    "    \n",
    "        if return_rmse == True:\n",
    "            return self.current_rmse\n",
    "        \n",
    "    def save_model(self, filename=\"./model/model\"):\n",
    "        assert type(self.model_back_bone) != type(None), \"can't save empty model, please load model\"\n",
    "        dump(self.model_back_bone, filename)\n",
    "    \n",
    "    def load_model(self, filename=\"./model/model\", return_model=False):\n",
    "        self.model_back_bone = load(filename)\n",
    "        if return_model:\n",
    "            return self.model_back_bone\n",
    "        \n",
    "    def init_model_from_supported_backbones(self, backbone_type):\n",
    "        assert backbone_type in self._supported_backbones.keys(), f\"The specified backbone type is not in the system please choose one of the following, {self._supported_backbones.keys()}.\"\n",
    "        self.model_back_bone = self._supported_backbones[backbone_type]\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "\n",
    "# Initialize a data pipline\n",
    "data_pipeline = AutochekDataProcessorPipeline(\n",
    "                                path_to_body_type=\"data/docs copy/bodytype.csv\", \n",
    "                                path_to_categories=\"data/docs copy/categories.csv\",\n",
    "                                path_to_condition=\"data/docs copy/condition.csv\",\n",
    "                                path_to_listing=\"data/docs copy/listing.csv\",\n",
    "                                path_to_trueprices=\"data/docs copy/trueprices.csv\",\n",
    "                )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# process incoming data\n",
    "data_pipeline.process_dataset()\n",
    "\n",
    "# #split merged data into train, test, val\n",
    "data_pipeline.split_data(return_split=False)\n",
    "\n",
    "# Fit Pipeline original data joined from different sources to extract categorical features.\n",
    "data_pipeline.pipeline_fit(data=\"original\", steps='all', return_result=False, only_normalize=False)\n",
    "\n",
    "# Fit Normalizer on train data.\n",
    "data_pipeline.pipeline_fit(data=\"trainset\", steps='all', return_result=False, only_normalize=True)\n",
    "\n",
    "# # save splitted data\n",
    "data_pipeline.save_splits(save_ordinary_processed_data=True, save_train_data=True, save_test_data=True, save_val_data=True)\n",
    "\n",
    "# save pipelines and normalizer\n",
    "data_pipeline.save_pipelines_nd_normalizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load and pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# load data\n",
    "data_pipeline.load_splits(load_ordinary_processed_data=True, load_train_data=True, load_test_data=True, load_val_data=True)\n",
    "\n",
    "# load pipeline\n",
    "data_pipeline.load_pipeline_nd_normalizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "\n",
    "# Transform Train data with pipeline from original data and normalizer from train data\n",
    "data_pipeline.pipeline_transform(data=\"trainset\", steps='all', return_result=False, normalize=True)\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X=data_pipeline.currentX, y=data_pipeline.currentY)\n",
    "# print(data_pipeline.currentX.shape, data_pipeline.currentY.shape)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "output = model1.predict(X=data_pipeline.currentX)\n",
    "train_rmse = mean_squared_error(data_pipeline.currentY, output, squared=False)\n",
    "# Output rmse for train data\n",
    "print(f\"Rmse for train is: {train_rmse} \")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rmse for train is: 43667074.335155904 \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Transform Train data with pipeline from original data and normalizer from train data\n",
    "data_pipeline.pipeline_transform(data=\"testset\", steps='all', return_result=False, normalize=True)\n",
    "output = model1.predict(X=data_pipeline.currentX)\n",
    "test_rmse = mean_squared_error(data_pipeline.currentY, output, squared=False)\n",
    "# Output rmse for test data\n",
    "print(f\"Rmse for test is: {test_rmse}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rmse for test is: 53553017.92216383\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# TODOs: \n",
    "# buildmethod to save data pipeline\n",
    "# build method to load pipeline\n",
    "# build method to save model\n",
    "# build method to load model\n",
    "# https://datastudio.google.com/reporting/ca0587ce-2ffc-4639-bd34-460b39036b9f\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}